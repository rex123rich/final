## 小Ｑ導盲機-影像辨識輔助裝置
# 小Ｑ導盲機-影像辨識輔助裝置
## 摘要  
         近年來隨著人口老齡化的情形加劇，未來盲人數量將成長數倍，而本計畫計畫設計一套透過隨手可得的手機來運
     行的盲人輔助系統，其主要包含四個組成要件:操作介面、環境判斷、資料處理、訊號輸出。操作介面主要目的為使盲
     人能輕易且方便的使用;環境判斷需要能判斷場景中所包含之人事物、顏色、表情;資料處理能夠將環境判斷出所包含
     之物件進行分析、運算而產生盲人能讀懂的語句;訊號輸出主要功能為以音訊的方式唸出產生的語句。此外以連續性感
     測周遭訊號的方式，來提醒使用者前方可能出現的突發事故。
       
## 研究動機
        隨著科技發展蓬勃，生活品質提升，但弱勢族群仍然沒有受到重視，我們團隊針對視力受損的人在一般生活中的一
    些小問題，想透過手機結合影像辨識幫助他們解決生活上不便之處並由此研究讓大家多注意弱勢族群。

## 研究目的
       為克服上述問題，本計劃旨在開發一套手機影 像辨識軟體與無障礙操作模組做結合，視障人士可以透過聲控操作本
    軟體，而系統則以人性化的聲音來引導視障人士解決生活上的困擾。目前該系統具備以下功能
   * 透過攝像機辨識圖片甚至是影像
   * 系統分析圖、影像之中所有物體的名稱
   * 以一句話敘述圖、影像中的情況，且用語音播出
 
## 所需技術
 * FireBase
     *  Bass(Backend as Service)，提供很豐富的後端服務，就像是Serverless架構一樣，讓Web、APP開發者能夠更專注在產品本身的開發上。
* AVFoundation
     *  一個具有Objective-C和Swift API的多媒體框架，它提供了用於在基於Apple Darwin的操作系統（iOS，macOS，tvOS和watchOS）上使用基於時間的視聽媒體的高級服務。
   
 * CoreML
     * 蘋果在WWDC 2017 發表的機器學習框架(Machine Learning Framework)，開發人員可以藉由Core ML的輔助，快速將已完成訓練的機器學習模型(model)整合進app中，讓app也能具備人工智慧的學習及預測能力。

 * YOLO v3
     * 目前對於物件偵測被最廣泛應用的一項技術，先在影像中框出bounding box選出懷疑候選的區域,再針對bounding box裡的資訊截取特徵值解析並分類。      
     
## 研究方法與步驟
* YOLO v3
    *       透過手機鏡頭傳送影像資料給YOLO v3判斷，產生數個String類型訊號        
* CoreML+深度學習
    *       將接收到的數個String訊號組成一有意義的句子，產生單個String類型訊號
* FireBase MLNL Translate
    *       將接收到的單個String訊號，英文翻成中文，產生單個String類型訊號
        
* AVFoundation
    *       將接收到的單個String訊號，轉成音訊並輸出
